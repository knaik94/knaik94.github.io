---
layout: post
title:  "Computer Security Notes for Exam 1"
date:   2017-02-20 
categories: CompSec
---

Exam 1 study guide
Paul Krzyzanowski
Modified by Karan Naik
Latest update: Sun Feb 20 2017

#Introduction


What is computer security?
protection of computers

Confidentiality
bob can not share alice's secrets

privacy
alice gives up privacy by sharing secret with bob
alice's privacy is intact with respect to others

Integrity (are you sure?) aka sureness
Data integrity (corruption?)
origin/destination (endpoint) integrity (no MITM)
system integrity (no loopholes, although physical methods always exist)
Availability (right information shown to right people)
Policy vs. Mechanism (theory/abstract vs day-to-day)

Vulnerability: you are the weakest link

Attack: calculated setup to achieve a goal through execution)
(rube goldberg machine)/(dominoes)

Threat categories (know but don't memorize)
Disclosure - spy espionage binoculars data dump
Deception - fake id changing mac address in aircrack-ng attack
Disruption - embargo or misdirection, fucking popups
Usurpation - overthrow king(kernel-level process) [hacking]



Snooping
Modification
Alteration
Repudiation of origin
Denial of receipt
Delay
Denial of service
You don't need to know the threat matrix or types of attackers/adversaries

What is a trusted computing base

Access control
What is protection?
What is access control?

Basic OS and hardware mechanisms for protection
Hardware timer
Process scheduler
Memory management unit (MMU)
user vs. kernel mode execution
You do not need to know about rings of privilege levels or call gates
Purpose of authentication
Subjects vs. objects
Unix access control model
root user
User, group, other
Read, write, execute permissions
Order of checking
You don't need to know windows domains
What do permissions mean for directories (e.g., execute?)?
You don't need to remember /etc/passwd and /etc/group but know that there's a file that stores a user' login, user ID, and a user's default group, and another that stores a list of groups and user names that belong to each group
You don't need to remember syntax but know:
chmod - change permissions of a file
chown - change ownership of a file
chgrp - change group of a file
umask - initial (default) permissions of a file
possible race condition with using umask
setuid - what does it do? What are the risks?
Access control lists (ACLs)
Know the purpose of ACLs
You don't need to know any syntax for setting them
Principle of least privilege
Privilege separation
Access control matrix
What's a row annd what's a column?
I will not ask about access transitions
I will not ask about domain transfers
Implementation problems of ACLs
Implementation problems of capability lists
Mandatory Access Control (MAC)
DAC vs. MAC
Bell-LaPadula Model
Simple Security Property
*-Property
Discretionary Security Property
I will not ask about Bell-LaPadula tranquility principles
Biba Integrity Model
Difference from Bell-LaPadula
Simple Integrity Property
*-Property
Type Enforcement (TE) Model
Just the concept
Role-Based Access Control (RBAC) Model
Just the concept
Multilateral security
What is the concept?
What is a compartment?
What does a lattice model represent?
Chinese Wall model
What is the goal?
What is a conflict class?
I will not ask you about the Simple security property and *-property but understand when a subject will or will not be granted access to an object.
I will not ask you about unsanitized data
Control hijacking
What is a buffer overflow?
Why are setuid programs attractive targets?
Stack overflow
Role of stack pointer
Return address overwrite
NOP slide = landing zone
Off-by-one stack overflow
What is a frame pointer?
Taking control (see hw assignment)
Heap overflow
What is the risk?
Format string attacks
What creates the vulnerability?
Understand how you can read the stack
Understand how you can write to memory via printf
Defenses
Safe vs. dangerous functions (e.g., strncpy vs. strcpy)
What does fuzzing do?
What's the problem with languages like C or C++?
Data Execute Protection
What does it do?
What is a return-to-libc attack?
What is Return Oriented Programming (ROP)?
Address Space Layout Randomization (ASLR)
What is it?
Stack canaries
What are they?
When are they checked?
Why would a compiler re-order how variables are allocated on the stack?
Other injection attacks (and a few other attacks)
Injection attacks
What are they?
SQL injection attack
I do not expect you to know SQL but recognize how the attack works and how interpreted languages and command interpreters in general are vulnerable
Prevention
Escaping (but error-prone)
Parameterized queries
Shell and execution environment attacks
IFS
Understand the concept of an internal field separator
How can a redefinition of IFS affect command parsing?
system() and popen() risks
Understand that the attacks are essentially the same as for SQL injection
PATH: what is the risk?
LD_PRELOAD/LD_LIBRARY_PATH: what is the risk?
How can you attack a program via a file descriptor?
Non-injection attacks
Understand what is meant by comprehension errors
Understand why parsing a filename can be tricky
Understand why unicode can make parsing even trickier
What is a homograph attack?
Understand it's for deception, not injection
What is a TOCTTOU (Time Of Check To Time Of Use) attack?
Relative Attack Surface Quotient (RASQ)
All you need to know is in the exam review slide deck - just understand bias as a multiplier to each possible attack type
Know what is meant by an attack vector
I will not ask you about root attack vectors, attack surfaces, or effective attack surface value
App Confinement
Why is access control not sufficient?
chroot
What does it do?
I will not ask you about jailkits
Why must it only be run as root?
How can you escape a chroot jail if you get root privileges?
FreeBSD Jails
Understand the goal and how they wanted to improve chroot
Ability to restric what root can do inside a jail
Linux app isolation
Namespaces
Recognize the different namespaces: IPC, network, mount (file system), process IDs, user/group IDs, network name
Understand the concept of the clone() system call: create a new process but specify which namespaces get shared
Capabilities
Note that these have no relation to capability lists for files
Know that they restrict what a proess can do with root privileges if it becomes root
You don't have to know the capabilities but it might be good to recognize a few in the notes
Control groups
Just know that they allow you to limit the amount of resources used by a proces (CPU, memory, file I/O, network I/O)
----------------------------------------------------------------------------


What is CompSec?
Protecting computers, programs, and data. 



Three objectives: Confidentiality, Integrity, Availability

Confidentiality - compartmentalization, privacy delineates

confidentiality refers to personal information shared with an attorney, physician, therapist, or other individual that generally cannot be divulged to third parties without the express consent of the client. 
privacy refers to the freedom from intrusion into one's personal matters, and personal information.

Privacy – right to keep things to yourself • Confidentiality – right to keep things about you from
being disclosed to others
• Security – protection of your personal information 

Confidentiality:people, Integrity:proccess, Availability:connections(to outside)


Confidentiality requires authorization in practice, privacy specifies boundries.


Confidentiality means that we do not make a system’s data and its resources available to everyone. Only authorized __ should have access. Privacy specifies limits on what information can be shared with others while confidentiality provides a means to block access to such information. Privacy is a reason for confidentiality. Someone being able to access a protected file containing your medical records without proper access rights is a violation of confidentiality. (HIPAA)


Integrity: hole in the pipeline/hole in the wall/fly on the wall/contamination
Integrity refers to the trustworthiness of a system. This means that everything is as you expect it to be: users are not imposters and processes are running correctly.

Data integrity: data not corrupted. (checksum)

Origin integrity: sender is vouched for, ssl.

Recipient integrity: receiver is intended target, not intercepted or imposter.

System integrity means that the entire computing system is working properly; that it has not been damaged or subverted. Processes are running the way they are supposed to.

Maintaining integrity means not just defending against intruders that want to modify a program or masquerade as others. It also means protecting the system against against accidental damage, such as from user or programmer errors.

Availability
Availability means that the system is available for use and performs properly. A denial of service (DoS) attack may not steal data or damage any files but may cause a system to become unresponsive.



Security is difficult. Software is incredibly complex. Large systems may comprise tens or hundreds of millions of lines of code. Systems as a whole are also complex. We may have a mix of cloud and local resources, third-party libraries, and multiple administrators. If security was easy, we would not have massive security breaches year after year. Microsoft wouldn’t have monthly security updates. There are no magic solutions … but there is a lot that can be done to mitigate the risk of attacks and their resultant damage.

We saw that computer security addressed three areas of concern. The design of security systems also has three goals.

Prevention (STOP before)
Prevention means preventing attackers from violating established security policies. It means that we can implement mechanisms into our hardware, operating systems, and application software that users cannot override – either maliciously or accidentally. Examples of prevention include enforcing access control rules for files and authenticating users with passwords.
Detection (IDENTIFY during)
Detection detects and reports security attacks. It is particularly important when prevention mechanisms fail. It is useful because it can identify weaknesses with certain prevention mechanisms. Even if prevention mechanisms are successful, detection mechanisms are useful to let you know that attempted attacks are taking place. An example of detection is notifying an administrator that a new user has been added to the system. Another example is being notified that there have been several consecutive unsuccessful attempts to log in.
Recovery(BACKUP) back that ass up
If a system is compromised, we need to stop the attack and repair any damage to ensure that the system can continue to run correctly and the integrity of data is preserved. Recovery includes forensics, the study of identifying what happened and what was damaged so we can fix it. An example of recovery is restoration from backups.
Security engineering is the task of implementing the necessary mechanisms and defining policies across all the components of the system. Like other engineering disciplines, designing secure systems involves making compromises. A highly secure system will be disconnected from any communication network, sit in an electromagnetically shielded room that is only accessible to trusted users, and run software that has been thoroughly audited. That environment is not acceptable for most of our computing needs. We want to download apps, carry our computers with us, and interact with the world. Even in the ultra-secure example, we still need to be concerned with how we monitor access to the room, who wrote the underlying operating system and compilers, and whether authorized users can be coerced to subvert the system. Systems have to be designed with some idea of who are likely potential attackers and what the threats are. This is called risk analysis.


the policy is the theory/model/framework
the mechanism is the implementaion



Secure systems have two parts to them: mechanisms and policies. A policy is a description of what is or is not allowed. For example, “users must have a password to log into the system” is a policy. Mechanisms* are used to implement and enforce policies. An example of a mechanism is the software that requests user IDs and passwords, authenticates the user, and allows entry to the system only if the correct password is used.

A vulnerability is a weakness in the security system. It could be a poorly defined policy, a bribed individual, or a flaw in the underlying mechanism that enforces security. An attack is a means of exploiting a vulnerability. For example, trying common passwords to log into a system is an attack. A threat is the potential harm from an attack on the system.

We refer to the trusted computing base (TCB) as the collection of hardware and software of a computing system that is critical to ensuring the system’s security. Typically, this is the operating system and system software. If the TCB is compromised, you no longer have assurance that any part of the system is secure. For example. the operating system may be modified to ignore the enforcement of file access permissions. If that happens, you no longer have assurance that any application is accessing files properly.

Threats fall into four broad categories:

Disclosure: Unauthorized access to data, which covers exposure, interception, interference, and intrusion. This includes stealing data, improperly making data available to others, or snooping on the flow of data.

Deception: Accepting false data as true. This includes masquerading, which is posing as an authorized entity; substitution or insertion of includes the injection of false data or modification of existing data; repudiation, where someone falsely denies receiving or originating data.

Disruption: Some change that interrupts or prevents the correct operation of the system. This can include maliciously changing the logic of a program, a human error that disables a system, an electrical outage, or a failure in the system due to a bug. It can also refer to any obstruction that hinders the functioning of the system.

Usurpation: Unauthorized control of some part of a system. This includes theft of service as well as any misuse of the system such as tampering or actions that result in the violation of system privileges.

Access control
See lecture notes



Program Hijacking (like dna splicing with ecoli)
Program hijacking refers to techniques that can be used to take control of a program and have it do something other than what it was intended to do. One class of techniques uses code injection, in which an adversary manages to add code to the program and change the program’s execution flow to run that code.

The best-known set of attacks are based on buffer overflow. Buffer overflow is the condition where a programmer allocates a chunk of memory (for example, an array of characters) but neglects to check the size of that buffer when moving data into it. Data will spill over into adjacent memory and overwrite whatever is in that memory.


array bounds! 

Languages such as C, C++, and assembler are susceptible to buffer overflows since the language does not have a means of testing array bounds. Hence, the compiler cannot generate code to validate that data is only going into the allocated buffer. For example, when you copy a string using strcpy(char *dest, char *src), you pass the function only source and destination pointers. The strcpy function has no idea how big either of the buffers are.

Stack-based overflows
When a process runs, the operating system’s program loader allocates a region for the executable code and static data (called the text and data segments), a region for the stack, and a region for the heap (used for dynamic memory allocation, such as by malloc).

Just before a program calls a function, it pushes parameters to the function on the stack prior to making the call. When the call is made, the return address gets pushed on the stack. On entry to the function that was called, the function pushes the current frame pointer (a register in the CPU) on the stack, which forms a linked list to the previous frame pointer and provides an easy way to revert the stack to where it was before making the function call. The frame pointer register is set to the current top of the stack. The function then allocates space on top of the stack to hold local variables and adjusts the stack pointer again. This enables the function to get interrupts or call other functions without overwriting anything useful on the stack. The compiler generates code to reference parameters and local variables as offsets from the current frame pointer register.

Before a function returns, the compiler generates code to:

adjust the stack back to point to where it was before the stack expanded to make room for local variables

restore the previous frame pointer by popping it off the stack (so that local variables for the previous function could be referenced properly)

At this time, the stack pointer points to a location on the stack that holds the return address Finally, the compiler generates code to return to the parent function.

Simple stack overflows
Local variables are allocated on the stack and the stack grows downward in memory. Hence, the top of the stack is in lower memory than the start, or bottom, of the stack. If a buffer (e.g., char buf[128]) is defined as a local variable, it will reside on the stack. As the buffer gets filled up, its contents will be written to higher and higher memory addresses. If the buffer overflows, data will be written further down the stack (in higher memory), overwriting the contents of any other variables that were allocated for that function and eventually overwriting the saved frame pointer and the saved return address.

When this happens and the function tries to return, the return address that is read from the stack will contain garbage data, usually a memory address that is not mapped into the program’s memory. As such, the program will crash when the function returns and tries to execute code at that invalid address. This is an availability attack. If we can exploit the fact that the program does not check the bounds of a buffer and overflows the buffer, we can cause a program to crash.

Subverting control flow through a stack overflow
Buffer overflow can be used in a more malicious manner. The buffer itself can be filled with bytes of valid machine code. If the attacker knows the exact size of the buffer, she can write just the right number of bytes to write a new return address into the very same region of memory on the stack that held the return address to the parent function. This new return address points to the start of the buffer that contains the injected code. When the function returns, it will “return” to the new code in the buffer and execute the code at that location.

Off-by-one stack overflows
As we saw, buffer overflow occurs because of programming bugs: the programmer neglected to make sure that the data written to a buffer does not overflow. This often occurs because the programmer used old, unsafe functions that do not allow the programmer to specify limits. Common functions include:

- strcpy(char *dest, char *src)

- strcat(char *dest, char *src)

- sprintf(char *format, ...)
Each of these functions has a safe counterpart that accepts a count parameter so that the function will never copy more than count number of bytes:

- strcpy(char *dest, char *src, int count)

- strcat(char *dest, char *src, int count)

- sprintf(char *format, int count,  ...)
You’d think this would put an end to buffer overflow problems. However, programmers may miscount or they may choose to write their own functions that do not check array bounds correctly. A common error is an off-by-one error. For example, a programmer may declare a buffer as:

char buf[128];
and then copy into it with:

for (i=0; i <= 128; i++)
    buf[i] = stuff[i];
Accidentally, the programmer used a <= comparison instead of <.

With off-by-one bounds checking, there is no way that malicious input can overwrite the return address on the stack: the copy operation would stop before that time. However, if the buffer is the first variable that is allocated on the stack, an off-by-one error can overwrite one byte of the saved frame pointer.

The potential for damage depends very much on what the value of that saved frame pointer was and how the compiler generates code for managing the stack. In the worst case, it could be set up to a value 255 bytes lower in memory. If the frame pointer is modified, the function will still return normally. However, upon returning, the compiler also pops the frame pointer from the stack. Now the program has a modified frame pointer. Any references to local variables will now be references in the buffer. Moreover, should that function return, it will update its stack pointer to this buffer area and return to an address that the attacker defined.

Heap overflows
Not all data is allocated on the stack: only local variables. Global and static variables are placed in a region of memory right above the executable program. Dynamically allocated memory (e.g., via new or malloc) comes from an area of memory called the heap. In either case, since this memory is not the stack, it does not contain return addresses so there is no ability for a buffer overflow attack to overwrite return addresses.

We aren’t totally safe, however. A buffer overflow will cause data to spill over into higher memory addresses above the buffer that may contain other variables. If the attacker knows how variables are allocated, they could be overwritten. While these overwrites will not change a return address, they can change things such as filenames, lookup tables, or linked lists. Some programs make extensive use of function pointers, which may be stored in global variables. If a buffer overflow can overwrite a function pointer then it can change the execution of the program: when that function is called, control will be transferred to a location of the attacker’s choosing.

If we aren’t sure of the exact address at which execution will start, we can fill a buffer with a bunch of NOP (no operation) instructions prior to the injected code. If the processor jumps anywhere in that region of memory, it will happily execute these NOP instructions until it eventually reaches the injected code. This is called a NOP slide, or a landing zone.

Format string attacks with printf
The family of printf functions are commonly used in C and C++ to create formatted output. They accept a format string that defines what will be printed, with % characters representing formatting directives for parameters. For example,

printf("value = %05d\n", v);
Will print a string such as

value = 01234
if the value of v is 1234.

Reading arbitrary memory
Occasionally, programs will use a format string that could be modified. For instance, the format string may be a local variable that is a pointer to a string. This local variable may be overwritten by a buffer overflow attack to point to a different string. Note that printf takes a variable number of arguments and matches each % directive in the format string with a parameter. If there are not enough parameters passed to printf, the function does not know that: it assumes they are on the stack and will happily read whatever value is on the stack where it thinks the parameter should be. This gives an attacker the ability to read arbitrarily deep into the stack. For example, a format string such as:

printf("%08x\n%08x\n%08x\n%08x\n");
will expect four parameters, all of which are missing. It will instead read the four values that are on the top of the stack and print each integer as an 8-character-long hexadecimal value prefixed with leading zeros (“%08x\n”).

Writing arbitrary memory
The printf function contains a somewhat obscure formatting directive: %n. Unlike other % directives that expect to read a parameter and format it, %n instead writes to the address corresponding to that parameter. It writes the number of characters that it has output thus far. For example,

printf(“paul%n says hi”, &printbytes);
will store the number 4 (strlen("paul")) into the variable printbytes. An attacker who can change the format specifier may be able to write to arbitrary memory. Each % directive to print a variable will cause printf to look for the next variable in the next slot in the stack. Hence, format directives such as %x, %lx, %llx will cause printf to skip over the length of an int, long, or long long and get the next variable from the following location on the stack. Thus, just like reading the stack, we can skip through any number of bytes on the stack until we get to the address where we want to modify a value. At that point, we insert a %n directive in the format string, which will modify that address on the stack with the number of bytes that were output. We can precisely control the value that will be written by specifying how many bytes are output as part of the format string. For example, a format of %.55000x tells printf to output a value to take up 55,000 characters. By using formats like that for output values, we can change the count that will be written with %n. Remember, we don’t care what printf actually prints; we just want to force the byte count to be a value we care about, such as the address of a function we want to call.

Defense against hijacking attacks
Better programming
Hijacking attacks are the result of sloppy programming: a lack of bounds checking that results in overflows. They can be eliminated if the programmer never uses unsafe functions (e.g., use strncpy instead of strcpy) and is careful about off-by-one errors.

A programer can use a technique called fuzzing to locate buffer overflow problems. Whenever a string can be provided by the user, the user will enter very long strings with well-defined patterns (e.g., “$$$$$$…”). If the app crashes because a buffer overflow destroyed a return address on the stack, the programmer can then load the core dump into a debugger and search for a substring of the entered pattern (“$$$$$”).

Buffer overflows can be avoided by using languages with stronger type checking and array bounds checking. Languages such as Java, C#, and Python check array bounds. C and C++ do not. However, it is sometimes difficult to avoid using C or C++.

Tight specification of requirements, coding to those requirements, and constructing tests based on those requirements helps avoid buffer overflow bugs. If input lengths are specified, they are more likely to be coded and checked. Documentation should be explicit, such as "user names longer than 32 bytes must be rejected”.

Data Execution Protection (DEP)
Buffer overflows affect data areas: either the stack, heap, or static data areas. There is no reason that those regions of code should contain executable code. Hence, it makes sense for the operating system to set the processor’s memory management unit (MMU) to turn off execute permission for memory pages in those regions.

This was not possible with early Intel processors: their MMU did not support enabling or disabling execute permissions. All memory could contain executable code. That changed in 2004, when Intel and AMD finally added an NX (no-execute) bit to their MMU. Operating system support followed. Windows, Linux, and macOS all currently support DEP.

DEP cannot always be used. Some environments, such as some LISP interpreters actually do need execution enabled in their stack and some need executable code in their heap section (to support dynamic loading and patching). DEP also does not guard against data modification attacks, such as heap-based overflows or some printf attacks.

DEP attacks
Attackers came up with some clever solutions to defeat DEP. The first of these is called return-to-libc*. Buffer overflows still allow us to corrupt the stack. We just cannot execute code on the stack. However, there is already a lot of code sitting in the program and the libraries it uses. Instead of adding code into the buffer, the attacker merely overflows a buffer to create a new return address and parameter list on the stack. When the function returns, it switches control to the new return address. This return address will be an address in the standard C library (libc), which contains functions such as printf, system, and front ends to system calls. All that an attacker often needs to do is to push parameters that point to a string in the buffer that contains a command to execute and then “return” to the libc system function, whose function is to execute a parameter as a shell command.

A more sophisticated variant of return-to-libc is Return Oriented Programming (ROP). Return oriented programming is similar to return-to-libc but realizes that execution can go to any arbitrary point in any function in any loaded library. The function will execute a series of instructions and eventually return. The attacker will overflow the stack with data that now tells this function where to “return”. Its return can jump to yet another arbitrary point in another library. When that returns, it can – once again – be directed to an address chosen by the intruder.

There are lots and lots of return instructions among all the libraries normally used by programs. Each of these tail ends of a function is called a gadget. It has been demonstrated that using carefully chosen gadgets allows an attacker to push a string of return addresses that will enable the execution of arbitrary algorithms. To make life easier for the attacker, tools have been created that search through libraries and identify useful gadgets. A ROP compiler then allows the attacker to program operations using these gadgets.

Address Space Layout Randomization
Stack overflow attacks require knowing and injecting an address that will be used as a target when a function returns. ROP also requires knowing addresses of all the entry points of gadgets. Address Space Layout Randomization (ASLR) is a technique that was developed to have the program loader pick random starting points for the executable program, static data, heap, stack, and shared libraries. Since code and data resides in different locations each time the program runs, the attacker is not able to program buffer overflows with useful known addresses. For ASLR to work, the program and all libraries must be compiled to use position independent code (PIC), which uses relative offsets instead of absolute memory addresses.

Stack canaries
A stack canary is a compiler technique to ensure that a function will not be allowed to return if a buffer overflow took place that may have clobbered the return address.

At the start of a function, the compiler adds code to generate a random integer (the canary) and push it onto the stack before allocating space for local variables. Before returning from a function, the compiler adds code that will check if the integer has the same value and has not been overwritten. If it has been changed, that is a sign that a buffer overflow took place. Since the canary sits between the return address on the stack and local variables, a change to the canary means that it is possible that values further down in the stack have been modified.

Injection and containment
We looked at buffer overflow and printf format string attacks that enable the modification of memory contents to change the flow of control in the program and, in the case of buffer overflows, inject executable binary code (machine instructions). Other injection attacks enable you to modify inputs used by command processors, such as interpreted languages or databases. We will now look at these attacks.

SQL Injection
It is common practice to take user input and make it part of a database query. This is particularly popular with web services, which are often front ends for databases. For example, we might ask the user for a login name and password and then create a SQL query:

sprintf(buf,
    ”SELECT * from logininfo WHERE username = '%s' AND password = '%s’;",
    uname, passwd);
Suppose that the user entered this for a password:

' OR 1=1 --
We end up creating this query string[1]:

SELECT * from logininfo WHERE username = 'paul' AND password = '' OR 1=1 -- ';
The “--” after “1=1” is a SQL comment, telling it to ignore everything else on the line. In SQL, OR operations have precendence over AND so the query checks for a null password (which the user probably does not have) or the condition 1=1, which is always true. In essence, the user’s “password” turned the query into one that ignores the user’s password and unconditionally validates the user.

Statements such as this can be even more destructive as the user can use semicolons to add multiple statements and perform operations such as dropping (deleting) tables or changing values in the database.

This attack can take place because the programmer blindly allowed user input to become part of the SQL command without validating that the user data does not change the quoting or tokenization of the query. A programmer can avoid the problem by carefully checking the input. Unfortunately, this can be difficult. SQL contains too many words and symbols that may be legitimate in other contexts (such as passwords) and escaping special characters, such as prepending backslashes or escaping single quotes with two quotes can be error prone as these escapes differ for different database vendors. The safest defense is to use parameterized queries, where user input never becomes part of the query but is brought in as parameters to it. For example, we can write the previous query as:

uname = getResourceString("username");
passwd = getResourceString("password");
query = "SELECT * FROM users WHERE username = @0 AND password = @1";
db.Execute(query, uname, passwd);
While SQL injection is the most common code injection attack, databases are not the only target. Creating executable statements built with user input is common in interpreted languages, such as Shell, Perl, PHP, and Python. Before making user input part of any invocable command, the programmer must be fully aware of parsing rules for that command interpreter.

Shell attacks
The various POSIX[2] shells (sh, csh, ksh, bash, tcsh, zsh) are commonly used as scripting tools for software installation, start-up scripts, and tying together workflow that involves processing data through multiple commands. A few aspects of how many of the shells work and the underlying program execution environment can create attack vectors.

IFS
The shell variable IFS, Internal Field Separator, defines set of characters that will be used as separators when parsing arguments. By default, IFS is set to space, tab, and newline. However, it can be set to anything else. This can change how the shell parses its input data without modifying the shell code. IFS can also affect output, depending on what is used to generate it. If a shell uses "$*" to expand a list of arguments, it will replace white space with the first character in the IFS variable.

More dangerously, IFS can cause the pathname separator character / to be treated as whitespace. A program might use a function such as this to send an email alert:

FILE *fp = popen("/usr/bin/mail –s \"system alert\"  user", "w");
The popen function simply launches a shell to execute the supplied string as a command and allows input to the command to be written to the FILE pointer fp. If the IFS variable is set to /, then instead of executing the command /usr/bin/mail, the shell will try to execute the command usr with parameters of bin, mail, … If an attacker can place a command called usr somewhere in the user’s search path then that command would be run instead.

system() and popen() functions
We just saw the popen function in action. Both system and popen are part of the Standard C Library and are common functions that C programmers use to execute shell commands. The system function runs a shell command while the popen function also runs the shell command but allows the programmer to capture its output and/or send it input via the returned FILE pointer.

We examined the vulnerability possible by changing IFS. In that example, the programmer would likely use a function such as snprintf to write the complete command with the proper user name into a buffer. This incurs the possibility of an injection attack if the input is not carefully validated. Suppose the command is created via:

char buf[bsize];
snprintf(cmd, "/usr/bin/mail -s \"system alert\" %s", bsize, username);
f = popen(cmd, "w");
If the attacker had the option to set the user name, she could enter a string such as:

nobody; rm -fr /home/*
which will result in popen running the following command:

sh -c "/usr/bin/mail -s \"system alert\" nobody; rm -fr /home/*"
which is a sequence of commands, the latter of which deletes all user directories.

Other environment variables
The shell PATH environment variable controls how the shell searches for commands. For instance, suppose

PATH=/home/paul/bin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/games
and the user runs the ls command. The shell will search through the PATH sequentially to find an executable filenamed ls:

/home/paul/bin/ls
/usr/local/bin/ls
/usr/sbin/ls
/usr/bin/ls
/bin/ls
/usr/local/games/ls
If an attacker can either change a user’s PATH environment variable or if one of the paths is publicly writable and appears before the “safe” system directories, then he can add a booby-trapped command in one of those directories. For example, if the user runs the ls command, the shell may pick up a booby-trapped version in the /usr/local/bin directory. Even if a user has trusted locations, such as /bin and /usr/bin foremost in the PATH, an intruder may place a misspelled version of a common command into another directory in the path. The safest remedy is to make sure there are no untrusted directories in PATH.

Some shells allow a user to set an ENV or BASH_ENV variable that contains the name of a file that will be executed as a script whenever a non-interactive shell is started (when a shell script is run, for example). If an attacker can change this variable then arbitrary commands may be added to the start of every shell script.

Shared library environment variables
In the distant past, programs used to be fully linked, meaning that all the code needed to run the program, aside from interactions with the operating system, was part of the executable program. Since so many programs use common libraries, such as the Standard C Library, they are not compiled into the code of an executable but instead are dynamically loaded when needed.

Similar to PATH, LD_LIBRARY_PATH is an environment variable used by the operating system’s program loader that contains a colon-separated list of directories where libraries should be searched. If an attacker can change a user’s LD_LIBRARY_PATH, common library functions can be overwritten with custom versions. The LD_PRELOAD environment variable allows one to explicitly specify shared libraries that contain functions that override standard library functions.

LD_LIBRARY_PATH and LD_PRELOAD will not give an attacker root access but they can be used to change the behavior of program or to log library interactions. For example, by overwriting standard functions, one may change how a program generates encryption keys, uses random numbers, sets delays in games, reads input, and writes output.

As an example, let’s suppose we have a trial program that checks the current time against a hard-coded expiration time:

#include <time.h>
#include <stdio.h>
#include <stdlib.h>

int
main(int argc, char **argv)
{
    unsigned long expiration = 1483228800;
    time_t now;

    /* check software expiration */
    now = time(NULL);
    if (time(NULL) > (time_t)expiration) {
        fprintf(stderr, "This software expired at %s", ctime(&expiration));
        fprintf(stderr, "This time is now %s", ctime(&now));
    }
    else
        fprintf(stderr, "You're good to go: %lu days left in your trial.\n",
            (expiration-now)/(60*60*24));
    return 0;
}
When run, we may get output such as:

$ ./testdate
This software expired at Sat Dec 31 19:00:00 2016
This time is now Fri Feb 17 21:32:03 2017
Let us write a replacement time function that always returns a fixed value that is less than the one we test for:

unsigned long time() {
    return (unsigned long) 1483000000;
}
We compile it into a shared library:

gcc -shared -fPIC time.c -o newtime.so
Now we set LD_PRELOAD and run the program:

$ export LD_PRELOAD=$PWD/newtime.so
$ ./testdate
You're good to go: 2 days left in your trial.
Note that our program now behaves differently and we never had to recompile it or feed it different data!

File descriptors
POSIX systems have a convention that programs expect to receive three open file descriptors when they start up:

file descriptor 0: standard input

file descriptor 1: standard output

file descriptor 2: standard error

Functions such as printf, scanf, puts, getc and others expect these file desciptors to be available for input and output. When a program opens a new file, the operating system searches through the file descriptor table and allocates the first available unused file descriptor. Typically this will be file descriptor 3. However, if any of the three standard file descriptors are closed, the operating system will use one of those as an available, unused file descriptor.

The vulnerability lies in the fact that we may have a program running with elevated privileges (e.g., setuid root) that modifies a file that is not accessible to regular users. If that program also happens to write to the user via, say, printf, there is an opportunity to corrupt that file. The attacker simply needs to close the standard output (file descriptor 1) and run the program. When it opens its secret file, it will be given file descriptor 1 and will be able to do its read and write operations on the file. However, whenever the program will print a message to the user, the output will not be seen by the user as it will be directed to what printf assumes is the standard output: file descriptor 1. Printf output will be written onto the secret file, thereby corrupting it.

The shell command (bash, sh, or ksh) for closing the standard output file is an obscure-looking >&-. For example:

./testfile >&-
Comprehension Errors
The overwhelming majority of security problems are caused by bugs or misconfigurations. Both often stem from comprehension errors. These are mistakes created when someone – usually the programmer or administrator – does not understand the details and every nuance of what they are doing. Some example include:

Not knowing all possible special characters that need escaping in SQL commands.

Not realizing that the standard input, output, or error file descriptors may be closed.

Not understanding how access control lists work or how to configure mandatory access control mechanisms such as type enforcement correctly.

If we consider the Windows CreateProcess function, we see it is defined as:

BOOL WINAPI CreateProcess(
  _In_opt_    LPCTSTR               lpApplicationName,
  _Inout_opt_ LPTSTR                lpCommandLine,
  _In_opt_    LPSECURITY_ATTRIBUTES lpProcessAttributes,
  _In_opt_    LPSECURITY_ATTRIBUTES lpThreadAttributes,
  _In_        BOOL                  bInheritHandles,
  _In_        DWORD                 dwCreationFlags,
  _In_opt_    LPVOID                lpEnvironment,
  _In_opt_    LPCTSTR               lpCurrentDirectory,
  _In_        LPSTARTUPINFO         lpStartupInfo,
  _Out_       LPPROCESS_INFORMATION lpProcessInformation);
We have to wonder whether a programmer who does not use this frequently will take the time to understand the ramifications of correctly setting process and thread security attributes, the current directory, environment, inheritance handles, and so on. There’s a good chance that the programmer will just look up an example on places such as github.com or stackoverflow.com and copy something that seems to work, unaware that there may be obscure side effects that compromise security.

As we will see in the following sections, comprehension errors also apply to the proper understanding of things as basic as various ways to express characters.

Directory parsing
Some applications, notably web servers, accept hierarchical filenames from a user but need to ensure that they restrict access only to files within a specific point in the directory tree. For example, a web server may need to ensure that no page requests go outside of /home/httpd/html.

An attacker may try to gain access by using paths that include .. (dot-dot), which is a link to the parent directory. For example, an attacker may try to download a password file by requesting

http://poopybrain.com/../../../etc/passwd
The hope is that the programmer did not implement parsing correctly and might try simply suffixing the user-requested path to a base directory:

"/home/httpd/html/" + "../../../etc/passwd"
to form

/home/httpd/html/../../../etc/passwd
which will retrieve the password file, /etc/passwd.

A programmer may anticipate this and check for dot-dot but has to realize that dot-dot directories can be anywhere in the path. This is also a valid pathname but one that should be rejected for trying to escape to the parent:

http://poopybrain.com/419/notes/../../416/../../../../etc/passwd
Moreover, the programmer cannot just search for .. because that can be a valid part of a filename. All three of these should be accepted:

http://poopybrain.com/419/notes/some..other..stuff/
http://poopybrain.com/419/notes/whatever../
http://poopybrain.com/419/notes/..more.stuff/
Also, extra slashes are perfectly fine in a filename, so this is acceptable:

http://poopybrain.com/419////notes///////..more.stuff/
The programmer should also track where the request is in the hierarchy. If dot-dot doesn’t escape above the base directory, it should most likely be accepted:

http://poopybrain.com/419/notes/../exams/
These are not insurmountable problems but they illustrate that a quick-and-dirty attempt at filename processing may be riddled with bugs.

Unicode parsing
If we continue on the example of parsing pathnames in a web server, let us consider a bug in early releases of Microsoft’s IIS (Internet Information Services, their web server). IIS had proper pathname checking to ensure that attempts to get to a parent are blocked:

http://www.poopybrain.com/scripts/../../winnt/system32/cmd.exe
Once the pathname was validated, it was passed to a decode function that decoded any embedded Unicode characters and then processed the request.

The problem with this technique was that non-international characters (traditional ASCII) could also be written as Unicode characters. A “/” could also be written in HTML as its hexadecimal value, %2f (decimal 47). It could also be represented as the two-byte Unicode sequence %c0%af.

The reason for this stems from the way Unicode was designed to support compatibility with one-byte ASCII characters. This encoding is called UTF–8. If the first bit of a character is a 0, then we have a one-byte ASCII character (in the range 0..127). However, if the first bit is a 1, we have a multi-byte character. The number of leading 1s determine the number of bytes that the character takes up. If a character starts with 110, we have a two-byte Unicode character.

With a two-byte character, the UTF–8 standard defines a bit pattern of

110a bcde   10fg hijk
The values a-k represent 11 bits that give us a value in the range 0..2047. The “/” character, 0x2f, is 47 in decimal and 0010 1111 in binary. The value represents offset 47 into the character table (called codepoint in Unicode parlance). Hence we can represent the “/” as 0x2f or as the two byte Unicode sequence:

1100 0000   1010 1111
which is the hexadecimal sequence %c0%af. Technically, this is disallowed. The standard states that codepoints less than 128 must be represented as one byte but the two byte sequence is supported by most Unicode parsers. We can also construct a valid three-byte sequence too.

Microsoft’s bug was that they ignored parsing %c0%af as being equivalent to a / because it should not have been used to represent the character. However, the Unicode parser was happy to translate it and attackers were able to use this to access any file in on a server running IIS. This bug also gave attackers the ability to invoke cmd.com, the command interpreter, and execute any commands on the server.

After Microsoft fixed the multi-byte Unicode bug, another problem came up. The parsing of escaped characters was recursive, so if the resultant string looked like a Unicode hexadecimal sequence, it would be re-parsed.

As an example of this, let’s consider the backslash (\), which Microsoft treats as equivalent to a slash (/) in URLs since their native pathname separator is a backlash[3].

The backslash can be written in a URL in hexadecimal format as %5c. The “%” character can be expressed as %25. The “5” character can be expressed as %35. The “c” character can be expressed as %63. Hence, if the URL parser sees the string %%35c, it would expand the %35 to the character “5”, which would result in %5c, which would then be converted to a \. If the parser sees %25%35%63, it would expand each of the %nn components to get the string %5c, which would then be converted to a \. As a final example, if the parser comes across %255c, it will expand %25 to % to get the string %5c, which would then be converted to a \.

It is not trivial to know what a name relates to but it is clear that all conversions have to be done before the validity of the pathname is checked. As for checking the validity of the pathname in an application, it is error-prone. The operating system itself parses a pathname a component at a time, traversing the directory tree and checking access rights as it goes along. The application is trying to recreate a similar action without actually traversing the file system but rather by just parsing the name and mapping it to a subtree of the file system namespace.

Homograph attacks
While we have been looking at issues resulting from Unicode, let us take a brief digression from system attacks and consider some deception attacks that are enabled by Unicode.

Unicode was designed to represent practically all of the world’s glyphs[4] and contains over 128,000 characters. It includes scripts for Latin, Greek, Cyrillic, Armenian, Hebrew, Arabic, Syriac, Thaana, Devanagari, Bengali, Gurmukhi, Oriya, Tamil, Telugu, Kannada, Malayalam, Sinhala, Thai, Lao, Tibetan, Myanmar, Georgian, Hangul, Ethiopic, Cherokee, Canadian Aboriginal Syllabics, Khmer, Mongolian, Han (Japanese, Chinese, Korean ideographs), Hiragana, Katakana, and Yi, as well as emojis and ancient scripts.

If we consider the lowly slash character, there are several variations with different representations:

 / = solidus (slash) = U+002F
 / = fraction slash = U+2044
 /  = division slash = U+2215
 ?  = combining short solidus overlay = U+0337
 ?  = combining long solidus overlay = U+0338
/ = fullwidth solidus = U+FF0F
Only one of these is a valid pathname separator (the solidus). Using others will create strings that look like pathnames but are not. Some characters may have multiple representations. For example, an accented a (á) is a distinct Unicode character, U+00C1, but also a two-character sequence, U+0041, U+0301. This is not a two-byte Unicode character but rather a combining accent followed by an “a”.

Situations like this make string comparisons a nightmare.

Moreover, some characters look similar. In the Latin alphabet, depending on the font, certain characters may look identical or similar. The number one (1), lowercase L (l), and capital i (I) can look virtually identical in some fonts. Zero (0) and the letter O may be confusing.

We can create a simple deception attack by registering the website paypai.com and writing the last letter as a capital I to create paypaI.com, which may confuse people with paypal.com in a phishing message.

The deception attack became more insidious with the introduction of internationalized domain names (IDN), which made Unicode characters valid elements of a domain name. While Unicode represents virtually all of the world’s scripts, may characters look identical in those scripts. For example the Greek letters A, B, and E (and many others!) look identical to the Latin A, B, and E as well as to the Cyrillic A, B, and E but have different encodings:

Latin	Greek	Cyrillic
A	U+0041	U+0391	U+0410
B	U+0042	U+0392	U+0412
E	U+0045	U+0395	U+0415
K	U+004B	U+039A	Ua041A
X	U+0058	U+03A7	U+0425
As an example, we can spell out wikipedia.org using the following non-Latin characters:

Cyrillic a (U+0430), e (U+435), p (U+0440)
Belarusian-Ukranian i (U+0456)
Or we can spell out paypal.com using Cyrillic lookalikes for p, a, and y.

TOCTTOU attacks
TOCTTOU stands for Time of Check to Time of Use. If we have code of the form:

if I am allowed to do something
    then do it
we may be exposing ourselves to a race condition. There is a window of time between the test and the action. If an attacker can change the condition after the check then the action may take place even if the check should have failed.

One example of this is the print spooling program, lpr. It runs as a setuid program with root privileges so that it can copy a file from a user’s directory into a privileged spool directory that serves as a queue of files for printing. Because it runs as root, it can open any file, regardless of permissions. To keep the user honest, it will check access permissions on the file that the user wants to print and then, only if the user has legitimate read access to the file, it will copy it over to the spool directory for printing. An attacker can create a link to a readable file and then run lpr in the background. At the same time, he can change the link to point to a file for which he does not have read access. If the timing is just perfect, the lpr program will check access rights before the file is re-linked but will then copy the file for which the user has no read access.

Another example of the TOCTTOU race condition is the set of temporary filename creation functions (tempnam, tempnam, mktemp, GetTempFileName, etc.). These functions create a unique filename when they are called but there is no guarantee that an attacker doesn’t create a file with the same name before that filename is used. If the attacker creates and opens a file with the same name, she will have access to that file for as long as it is open, even if the user’s program changes access permissions for the file later on.

The best defense for the temporary file race condition is to use the mkstemp function, which creates a file based on a template name and opens it as well, avoiding the race condition between checking the uniqueness of the name and opening the file.

Metrics
It is challenging to establish just how secure a piece of software is. We can search for bugs that we know about but we don’t know where the next bug that can compromise security is lurking. In general, the fewer opportunities for attack that we give the adversary, the more likely our code is to be secure. We want to minimize interactions with outside elements: with users, files, and sockets. Any interactions may be attack targets, such as improper access controls, changed files, bad inputs, and network protocol attacks. All interactions must be given special attention and carefully validated and monitored.

Microsoft attempted to create a general metric to assess whether one piece of software is more likely to be vulnerable than another. It is called the Relative Attack Surface Quotient, or RASQ. Very roughly, this is a weighted measure of the various interactions of a program. An attack surface identifies how exposed a system is to attacks: it is the set of all possible interactions in a program with the outside. An attack vector is the set of software by which an attacker may carry out an attack (e.g., a web application).

RASQ looks at “data elements” that may pose security risks, such as open ports, named pipes, RPC endpoints, number of installed services, number of services running as SYSTEM (root in Unix terms), number of users, etc. Each of these is treated as the root vector, the primary mechanism by which an adversary may attack the system.

RASQ basically multiplies each of these root vectors by a bias, an estimate of how harmful that particular attack may be to the system. It then sums up all of these products for all possible vectors in the system to get a final score. The higher the score, the more likely it is that the system will have vulnerabilities.

For example, open sockets has the highest possible value of 1.0 since it is an easy target for remote attacks. Enabled accounts have a bias of 0.7 since default accounts on a system make brute-force password attacks easier, but you still need to get past password authentication to carry out an attack. Weak ACLs in a local file system, on the other hand, have a bias of only 0.2 since files in a system become targets only after a system is compromised.

App confinement
Two lessons we learned from experience are that applications can be compromised and that applications may not always be trusted. Server applications, in particular, such as web servers and mail servers have been compromised over and over again. This is particularly harmful as they often run with elevated privileges on systems on which normal users do not have accounts. The second category of risk is that we may not always trust an application. We trust our web server to work properly but we cannot necessarily trust that the game we downloaded from some unknown developer will not try to upload our files, destroy our data, or try to change our system configuration.

With this resignation to security in mind, we need to turn our attention to limiting the resources available to an application and making sure that a misbehaving application cannot harm the rest of the system. These are the goals of confinement.

Our initial thoughts to achieving confinement may involve proper use of access controls. For example, we can run server applications as low-privilege users and make sure that we have set proper read/write/execute permissions on files, read/write/search permissions on directories, or even set up role-based policies.

However, access controls usually do not give us the ability to set permissions for don’t allow access to anything else. For example, we may want our web server to have access to all files in /home/httpd but nothing outside of that directory. Access controls do not let us express that rule. Instead, we are responsible for changing the protections of every file on the system and making sure it cannot be accessed by “other”. We also have to hope that no users change those permissions. In essence, we must disallow the ability for anyone to make files publicly accessible because we never want our web server to access them. We may be able to use mandatory access control mechanisms if they are available but, depending on the system, we may not be able to restrict access properly either. More likely, we will be at risk of comprehension errors and be likely to make a configuration error, leaving parts of the system vulnerable. To summarize, even if we can get access controls to help, we will not have high assurance that they do.

Access controls also only focus on protecting access to files and devices. A system has other resources, such as CPU time, memory, disk space, and network. We may want to control how much of all of these an application is allowed to use. POSIX systems provide a setrlimit system call that allows one to set limits on certain resources for the current process and its children. These controls include the ability to set file size limits, CPU time limits, various memory size limits, iand maximum number of open files.

We also may want to control the network identity for an application. All applications share the same IP address on a system but this may allow a compromised application to exploit address-based access controls. For example, you may be able to connect to or even log into system that believe you are a trusted computer. An exploited application may end up confusing network intrusion detection systems.

Just limiting access through resource limits and file permissions is also insufficient for services that run as root. If an attacker can compromise an app and get root access to execute arbitrary functions, she can change resource limits (just call setrlimit with different values), change any file permissions, and even change the IP address and domain name of the system.

In order to truly confine an application, we would like to create a set of mechanisms that enforce access controls to all of a system’s resources, are easy to use so that we have high assurance in knowing that the proper restrictions are in place, and work with a large class of applications. We can’t quite get all of this yet but we can come close.

chroot
The oldest app confinement mechanism is Unix’s chroot system call and command, originally introduced in 1979 in the seventh edition[5]. The chroot system call changes the root directory of the calling process to the directory specified as a parameter.

chroot("/home/httpd/html");
Sets the root of the file system to /home/httpd/html for the process and any processes it creates. The process cannot see any files outside that subset of the directory tree. This isolation is often called a chroot jail.

Jailkits
If you run chroot, you will likely get an error along the lines of:

# chroot newroot
chroot: failed to run command ‘/bin/bash’: No such file or directory
This is because /bin/bash is not within the root (in this case, the newroot directory). You’ll then create a bin subdirectory and try running chroot again and get the same error:

# mkdir newroot/bin
# ln /bin/bash newroot/bin/bash
# chroot newroot
chroot: failed to run command ‘/bin/bash’: No such file or directory
You’ll find that is also insufficient and that you’ll need to bring in the shared libraries that /bin/bash needs by mounting /lib, /lib64, and /usr/lib within that root just to enable the shell to run. Otherwise, it cannot load the libraries it needs since it cannot see above its root (i.e., outside its jail). To simplify this process, a jailkit simplifies the process of setting up a chroot jail by providing a set of utilities to make it easier to create the desired environment within the jail and populate it with basic accounts, commands, and directories.

Problems with chroot
Chroot only limits access to the file system namespace. It does not restrict access to resources and does not protect the machine’s network identity. Applications that are compromised to give the attacker root access make the entire system vulnerable since the attacker has access to all system calls.

Chroot is available only to administrators. If this was not the case then any user would be able to get root access within the chroot jail. You would: 1. Create a chroot jail 2. Populate it with the shell program and necessary support libraries 3. Link the su command (set user, which allows you to authenticate to become any user) 4. Create password files within the jail with a known password for root. 5. Use the chroot command to enter the jail. 6. Run su root to become the root user. The command will prompt you for a password and validate it against the password file. Since all processes run within the jail, the password file is the one you set up.

You’re still in the jail but you have root access.

Escaping from chroot
If someone manages to compromise an application running inside a chroot jail and become root, they are still in the jail but have access to all system calls. For example, they can send signals to kill all other processes or shut down the system. This would be an attack on availability.

Attaining root access also provides a few ways of escaping the jail. On POSIX systems, all non-networked devices are accessible as files within the filesystem. Even memory is accessible via a file (/dev/mem). An intruder in a jail can create a memory device (character device, major number = 1, minor number = 1):

mknod mem c 1 1
With the memory device, the attacker can patch system memory to change the root directory of the jail. More simply, an attacker can create a block device with the same device numbers as that of the main file system. For example, the root file system on my Linux system is /dev/sda1 with a major number of 8 and a minor number of 1. An attacker can recreate that in the jail:

mknod rootdisk b 8 1
and then mount it as a file system within the jail:

mount -t ext4 rootdisk myroot
Now the attacker, still in the jail, has full access to the entire file system, which is as good as being out of the jail. He can add user accounts, change passwords, delete log files, run any commands, and even reboot the system to get a clean login.

FreeBSD Jails
Chroot was good in confining the namespace of an application but useless against providing security if an application had root access and did nothing to restrict access to other resources.

FreeBSD Jails are an enhancement to the idea of chroot. Jails provide a restricted filesystem namespace, just like chroot does, but also place restrictions on what processes are allowed to do within the jail, including selectively removing privileges from the root user in the jail. For example, processes within a jail may be configured to:

Bind only to sockets with a specified IP address and specific ports
Communicate only with other processes within the jail and none outside
Not be able to load kernel modules, even if root
Have restricted access to system calls that include:
Ability to create raw network sockets
Ability to create devices
Modify the network configuration
Mount or unmount filesystems
FreeBSD Jails are a huge improvement over chroot since known escapes, such as creating devices and mounting filesystems and even rebooting the system are disallowed. Depending on the application, policies may be coarse. The changed root provides all or nothing access to a part of the file system. This does not make Jails suitable for applications such as a web browser, which may be untrusted but may need access to files outside of the jail. Think about web-based applications such as email, where a user may want to upload or download attachments. Jails also do not prevent malicious apps from accessing the network and trying to attack other machines … or from trying to crash the host operating system. Moreover, FreeBSD Jails is a BSD-only solution. With an estimated 0.95…1.7% share of server deployments, it is a great solution on an operating system that is not that widely used.

Linux namespaces, cgroups, and capabilities
Linux’s answer to FreeBSD Jails is a combination of three elements: control groups, namespaces, and capabilities.

Control groups (cgroups)
Linux control groups, also called cgroups, allow you to allocate resources such as CPU time, system memory, disk bandwidth, network bandwidth, and the ability to monitor resource usage among user-defined groups of processes. This allows, for example, an administrator to allocate a larger share of the processor to a critical server application.

An administrator creates one or more cgroups and assigns resource limits to each of them. Then any application can be assigned to a control group and will not be able to use more than the resource limits configured in that control group. Applications are unaware of these limits. Control groups are organized in a hierarchy similar to processes. Child cgroups inherit some attributes from the parents.

Linux namespaces
Chroot only changed the filesystem namespace. That is the best known namespace in the system but not the only one. Linux namespaces expand the chroot concept to provide control over how processes see the following namespaces:

Namespace	Description	Controls
IPC	System V IPC, POSIX message queues	Objects created in an IPC namespace are only visible to other processes in that namespace (CLONE_NEWIPC)
Network	Network devices, stacks, ports	Isolates IP protocol stacks, IP routing tables, firewalls, socket port numbers ( CLONE_NEWNET)
Mount	Mount points	A set of processes can have their own distinct mount points and view of the file system (CLONE_NEWNS)
PID	Process IDs	Processes in different PID namespaces can have their process IDs – the child cannot see parent processes or other namespaces (CLONE_NEWPID)
User	User & group IDs	Per-namespace user/group IDs. Also, you can be root in a namespace but have restricted privileges ( CLONE_NEWUSER )
UTS	host name and domain name	setting hostname and domainname will not affect rest of the system (CLONE_NEWUTS)
Cgroup	control group	Sets a new control group for a process (CLONE_NEWCGROUP)
A process can dissociate any or all of these namespaces from its parent via the unshare system call. For example, by unsharing the PID namespace, a process gets a no longer sees other processes and will only see itself and any child processes it creates.

The Linux clone system call is similar to fork in that it creates a new process. However, it allows you to pass flags that will specify which parts of the execution context will be shared with the parent. For example, a cloned process may choose to share memory and open file descriptors, which will make it behave like threads. It can also choose to share – or not – any of the elements of the namespace.

Capabilities
A problem that FreeBSD Jails tackled was that of restricting the power of root inside a Jail. You could be a root user but still disallowed from executing certain system calls. Linux capabilities[6] tackle this issue as well. Traditionally, Unix systems distinguished privileged versus unprivileged processes. Privileged processes were those that ran with a user ID of 0, called the root user. When running as root, the operating system would allow access to all system calls and all file permission checks were bypassed. You could do anything.

Linux capabilities identify groups of operations, called capabilities, that can be controlled independently on a per-thread basis. The list is somewhat long, 38 controls, and includes capabilities such as:

CAP_CHOWN: make arbitrary changes to file UIDs and GIDs
CAP_DAC_OVERRIDE: bypass read/write/execute checks
CAP_KILL: bypass permission checks for sending signals
CAP_NET_ADMIN: network management operations
CAP_NET_RAW: allow RAW sockets
CAP_SETUID: arbitrary manipulation of process UIDs
CAP_SYS_CHROOT: enable chroot
The kernel keeps track of four capability sets for each thread. A capability set is a list of zero or more capabilities. The sets are:

Permitted: If a capability is not in this set, the thread or its children can never require that capability. This limits the power of what a process and its children can do.

Inheritable: These capabilities will be inherited when a thread calls execve to execute a program (POSIX programs are executed with the same thread; we are not creating a new process)

Effective: This is the current set of capabilities that the thread is using. The kernel uses these to perform permission checks.

Ambient: This is similar to Inheritable and contains a set of capabilities that are preserved across an execve of a program that is not privileged. If a setuid or setgid program is run, will clear the ambient set. These are created to allow a partial use of root features in a controlled manner. It is useful for user-level device drivers or software that needs a specific privilege (e.g., for certain networking operations).

A child process created via fork (the standard way of creating processes) will inherit copies of its parents capability sets following the rules of which capabilities have been marked as inheritable.

From a security point of view, the key concept of capabilities is that they allow us to provide limited elevation of privileges to a process. A process may become root but can still be severely limited in what it can do. For example. we can take away the ability mount filesystems or reconfigure the network.

The Linux combination of cgroups, namespaces, and capabilities provides a powerful set of mechanisms to

Set limits on the system resources (processor, disk, network) that a group of processes will use.

Constrain the namespace, making parts of the filesystem or the existence of other processes or users invisible.

Limit the operations available to processes, even if they are granted root privileges.

This enables us to create stronger jails and have a fine degree of control as to what processes are or are not allowed to do in that jail.

While bugs have been found these mechanisms, the more serious problem is that of comprehension. The system has become far, far more complex than it was in the days of chroot. A user has to learn quite a lot to use these mechanisms properly. Failure to understand their behavior fully can create vulnerabilities. For example, namespaces do not prohibit a process from making privileged system calls. They simply limit what a process can see. A process may not be able to send a kill signal to another process only because it does not share the same process ID namespace.

Together with capabilities, namespaces allow a restricted environment that also places limits on the abilities to perform operations even if a process is granted root privileges. This enables ordinary users to create namespaces. You can create a namespace and even create a process running as a root user (UID 0) within that namespace but it will have no capabilities beyond those that were granted to the user.

References
Injection
SQL Injection, The Open Web Application Security Project, April 10, 2016.

SQL Injection, Acunetix.

Simson Garfinkel & Gene Spafford, Section 11.5, Protecting Yourself, Practical UNIX & Internet Security, Second Edition, April 1996. Discusses shell attacks.

Directory traversal attack, Wikipedia.

Why does Directory traversal attack %C0%AF work?, Information Security Stack Exchange, September 9, 2016

Tom Rodriquez, What are unicode vulnerabilities on Internet Information Server (IIS)?, SANS.

The Unicode Consortium.

IDN homograph attack, Wikipedia.

Time of check to time of use, Wikipedia.

Michael Cobb, How to mitigate the risk of a TOCTTOU attack, TeachTarget, August 2011.

Ernst & Yount LLP Security & Technology Solutions, Using Attack Surface Area And Relative Attack Surface Quotient To Identify Attackability. Customer Information Paper.

Michael Howard, Back to the Future: Attack Surface Analysis and Reduction, Microsoft Secure Blog, February 14, 2011.

Olivier Sessink, Jailkit, November 18, 2015.

Confinement
Evan Sarmiento, Chapter 4. The Jail Subsystem, FreeBSD Architecture Handbook, The FreeBSD Documentation Project. 2001, Last modified: 2016–10–29.

Matteo Riondato, Chapter 14. System Administration: Jails, FreeBSD Architecture Handbook, The FreeBSD Documentation Project. 2001, Last modified: 2016–10–29.

Chapter 1. Introduction to Control Groups, Red Hat Enterprise Linux 6.8 Resource Management Guide.

Note that sprintf is vulnerable to buffer overflow. We should use snprintf, which allows one to specify the maximum size of the buffer.  ?

Unix, Linux, macOS, FreeBSD, NetBSD, OpenBSD, Android, etc.  ?

the official Unicode name for the slash and backslash characters are solidus and reverse solidus, respectively.  ?

a glyph is a printable character. Unicode is designed around the concept of scripts rather than languages since multiple languages often share the same set of scripts.  ?

Note that Wikipedia and many other sites refer to this as “Version 7 Unix”. Unix has been under continuous evolution at Bell Labs from 1969 through approximately 1989. As such, it did not have versions. Instead, an updated set of manuals was published periodically. Installations of Unix have been referred to by the editions of their manuals.  ?

Linux capabilities are not to be confused with the concept of capability lists, which are a form of access control that Linux does not use).  ?

